# ğŸš€ DOC RAG Server Backend â€” FastAPI + Qdrant (Amazon Export)

Build a local, privacyâ€‘friendly RAG chatbot over your Amazon data export using FastAPI, Qdrant, and OpenAI. Ingest your CSV/JSON files, search semantically, and ask questions with context. ğŸ§ ğŸ”ğŸ’¬

Why Qdrant? ğŸ¤
- Vector database purposeâ€‘built for similarity search
- Open source, fast HNSW index, great local DX
- Simple API and client SDK, easy Docker setup

What you get ğŸ“¦
- Ingestion of `AMAZON_DATA` into Qdrant with embeddings
- Chat endpoint that answers based on retrieved context
- Qdrant utility endpoints (list/info/count/recreate/delete collections)
- Docker Compose to run everything locally
- Scripts and curl examples to verify quickly

Quickstart (Docker Compose) ğŸ³
- Copy `.env.example` â†’ `.env`.
- If you use the builtâ€‘in Qdrant service, keep `QDRANT_URL=http://qdrant:6333` and start with profile: `docker compose --profile with-qdrant up -d --build`.
- If you already have Qdrant on the host (port 6333), set `QDRANT_URL=http://host.docker.internal:6333`.
- Start and autoâ€‘ingest: `scripts/start-compose.sh` (API is on `http://localhost:8010`).
- Or manually: `docker compose up -d --build` â†’ `make ingest` â†’ `make chat` (API on `http://localhost:8010`).

Quickstart (Python host) ğŸ
- Create `.env` (see `.env.example`). Set `QDRANT_URL=http://localhost:6333`.
- Start Qdrant: `docker run -p 6333:6333 -v qdrant_storage:/qdrant/storage qdrant/qdrant`.
- Install deps: `pip install -r requirements.txt`.
- Run API: `uvicorn app.main:app --reload`.

Environment ğŸ§© (see `.env.example`)
- `QDRANT_URL`, `QDRANT_API_KEY` (optional), `QDRANT_COLLECTION` (default `amazon_export`)
- `EMBEDDING_MODEL` (default `sentence-transformers/all-MiniLM-L6-v2`)
- `OPENAI_API_KEY` and optional `OPENAI_MODEL` (e.g., `gpt-4o-mini`)
- `AMAZON_DATA_PATH` (default `AMAZON_DATA`)
- Ollama optional (leave unset if not used)

API Endpoints ğŸŒ (Compose default base: `http://localhost:8010`)
- `GET /health` â€” Health check
- `POST /ingest` â€” Scan `AMAZON_DATA` and upsert vectors into Qdrant
- `POST /chat` â€” RAG chat with topâ€‘K Qdrant context (OpenAI or Ollama)
- `GET /search?q=...&k=5` â€” Debug semantic search (no LLM)

Qdrant Utility Endpoints ğŸ§±
- `GET /qdrant/collections` â€” List collections (names)
- `GET /qdrant/collections/{name}` â€” Collection info + approximate vector count
- `GET /qdrant/count?name={optional}` â€” Count points in a collection (defaults to active)
- `DELETE /qdrant/collections/{name}` â€” Delete a collection
- `POST /qdrant/collections/{name}/recreate` â€” Recreate collection with current embedding dim + cosine

Test with curl ğŸ§ª (Compose uses port 8010)
```bash
# Ingest your Amazon export
curl -X POST http://localhost:8010/ingest

# Ask a question (German example)
curl -X POST http://localhost:8010/chat \
  -H 'Content-Type: application/json' \
  -d '{"message":"Welche Bestellungen habe ich 2021 gemacht?","top_k":5}'

# Inspect nearest neighbors only
curl 'http://localhost:8010/search?q=Meine%20Bestellung%20Buch&k=5'

# Qdrant collections
curl http://localhost:8010/qdrant/collections
curl http://localhost:8010/qdrant/collections/amazon_export
curl http://localhost:8010/qdrant/count
```

How it works âš™ï¸
- Ingestion reads `.csv` and `.json`, flattens rows/items to compact text blocks.
- Embeds with `all-MiniLM-L6-v2` (384â€‘D) and upserts into Qdrant (cosine).
- Chat retrieves topâ€‘K nearest contexts and prompts the LLM to answer only from them.

Troubleshooting ğŸ§°
- Qdrant unreachable: check `QDRANT_URL`, container health (`docker compose ps`), or port.
- API 401 from Qdrant: set `QDRANT_API_KEY` in both Qdrant and `.env`.
- Old collection dimension mismatch: `DELETE /qdrant/collections/{name}` then reâ€‘`/ingest`.

# RAG Backend with NestJS and Pinecone

[![NestJS](https://img.shields.io/badge/NestJS-E0234E?style=flat&logo=nestjs&logoColor=white)](https://nestjs.com/)
[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Pinecone](https://img.shields.io/badge/Pinecone-000000?style=flat&logo=pinecone&logoColor=white)](https://www.pinecone.io/)
[![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=flat&logo=openai&logoColor=white)](https://openai.com/)

Ein vollstÃ¤ndiges **RAG (Retrieval-Augmented Generation)** Backend-System basierend auf **NestJS** mit **Pinecone** Vector Database und **OpenAI** Integration.

## ğŸš€ Features

- **ğŸ“„ Document Upload & Processing**: UnterstÃ¼tzung fÃ¼r PDF, TXT, DOCX und Bilddateien (JPG, PNG, GIF, BMP, TIFF, WEBP) mit OCR
- **ğŸ” Semantic Search**: Vektor-basierte Dokumentensuche mit Pinecone
- **ğŸ¤– AI Question Answering**: Kontextuelle Antworten mit OpenAI GPT Modellen
- **ğŸŒ¿ Cannabis Strain Recommendations**: AI-powered mood-basierte Cannabis-Empfehlungen mit Knowledge Graph-Ã¤hnlicher FunktionalitÃ¤t
- **ğŸ§  Cognee Knowledge Graph**: Erweiterte semantische Datenverarbeitung und EntitÃ¤ts-Extraktion
- **ğŸ“Š Swagger API Documentation**: VollstÃ¤ndige API-Dokumentation
- **âš¡ Rate Limiting**: Schutz vor API-Missbrauch
- **ğŸ”’ Input Validation**: Robuste Datenvalidierung mit class-validator
- **ğŸ“ˆ Performance Monitoring**: Logging und Metriken fÃ¼r alle Requests
- **ğŸ›¡ï¸ Error Handling**: Graceful Behandlung aller API-Fehler

## ğŸ“ Projektstruktur

```
rag-backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.module.ts          # Haupt-Anwendungsmodul
â”‚   â”œâ”€â”€ main.ts               # Anwendungs-Einstiegspunkt
â”‚   â”œâ”€â”€ documents/            # Document Management
â”‚   â”‚   â”œâ”€â”€ documents.module.ts
â”‚   â”‚   â”œâ”€â”€ documents.controller.ts
â”‚   â”‚   â”œâ”€â”€ documents.service.ts
â”‚   â”‚   â””â”€â”€ dto/             # Data Transfer Objects
â”‚   â”œâ”€â”€ embeddings/          # Text Embeddings Service
â”‚   â”‚   â”œâ”€â”€ embeddings.module.ts
â”‚   â”‚   â””â”€â”€ embeddings.service.ts
â”‚   â”œâ”€â”€ pinecone/           # Pinecone Vector Database
â”‚   â”‚   â”œâ”€â”€ pinecone.module.ts
â”‚   â”‚   â””â”€â”€ pinecone.service.ts
â”‚   â”œâ”€â”€ llm/               # Large Language Model
â”‚   â”‚   â”œâ”€â”€ llm.module.ts
â”‚   â”‚   â””â”€â”€ llm.service.ts
â”‚   â”œâ”€â”€ query/             # Query & RAG Functionality
â”‚   â”‚   â”œâ”€â”€ query.module.ts
â”‚   â”‚   â”œâ”€â”€ query.controller.ts
â”‚   â”‚   â”œâ”€â”€ query.service.ts
â”‚   â”‚   â””â”€â”€ dto/
â”‚   â”œâ”€â”€ cannabis/          # Cannabis Strain Recommendations ğŸŒ¿
â”‚   â”‚   â”œâ”€â”€ cannabis.module.ts
â”‚   â”‚   â”œâ”€â”€ cannabis.controller.ts
â”‚   â”‚   â”œâ”€â”€ cannabis.service.ts
â”‚   â”‚   â””â”€â”€ dto/           # Cannabis-specific DTOs
â”‚   â””â”€â”€ cognee/           # Cognee Knowledge Graph ğŸ§ 
â”‚       â”œâ”€â”€ cognee.module.ts
â”‚       â”œâ”€â”€ cognee.controller.ts
â”‚       â”œâ”€â”€ cognee.service.ts
â”‚       â””â”€â”€ dto/           # Cognee-specific DTOs
â”œâ”€â”€ package.json
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ› ï¸ Installation & Setup

### 1. Projekt klonen und Dependencies installieren

```bash
git clone <repository-url>
cd rag-backend
npm install
```

### 2. Environment Variables konfigurieren

Kopiere `.env.example` zu `.env` und fÃ¼lle die erforderlichen Werte aus:

```bash
cp .env.example .env
```

```env
# Server Configuration
PORT=3000

# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=rag-documents

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
EMBEDDING_MODEL=text-embedding-3-small

# Replicate Configuration
REPLICATE_API_TOKEN=your-token

# Cognee Knowledge Graph Configuration
COGNEE_API_KEY=your_cognee_api_key_here
COGNEE_BASE_URL=https://api.cognee.ai/v1

# Billing / Stripe Configuration
STRIPE_SECRET_KEY=sk_test_...
STRIPE_PRICE_ID=price_...
STRIPE_WEBHOOK_SECRET=whsec_...
FRONTEND_URL=http://localhost:5173
```

### 3. Pinecone Index erstellen

1. Gehe zu [Pinecone Console](https://app.pinecone.io/)
2. Erstelle einen neuen Index mit folgenden Einstellungen:
   - **Name**: `rag-documents` (oder wie in `.env` konfiguriert)
   - **Dimensions**: `1024` (fÃ¼r text-embedding-3-small)
   - **Metric**: `cosine`
   - **Pod Type**: `starter` (fÃ¼r kostenlose Version)

### 4. API Keys konfigurieren

#### OpenAI API Key:
1. Gehe zu [OpenAI Platform](https://platform.openai.com/api-keys)
2. Erstelle einen neuen API Key
3. FÃ¼ge ihn in deine `.env` Datei ein

#### Cognee API Key:
1. Registriere dich bei [Cognee Platform](https://cognee.ai) 
2. Generiere einen API Key im Dashboard
3. FÃ¼ge `COGNEE_API_KEY` in deine `.env` Datei ein
4. Optional: Konfiguriere `COGNEE_BASE_URL` falls du eine eigene Cognee-Instanz nutzt

### 5. Anwendung starten

```bash
# Development
npm run start:dev

# Production
npm run build
npm run start:prod
```

Die API lÃ¤uft unter: `http://localhost:3000`
Swagger Dokumentation: `http://localhost:3000/api`

## ğŸ“š API Endpoints

### Health Check
- `GET /api/v1` - API Status
- `GET /api/v1/version` - Version Information
- `GET /api/v1/query/health` - Service Health Check

### Billing & Tokens
- `GET /api/v1/tokens` â€“ Aktuelles Token-Guthaben fÃ¼r anonymen Client (Header `X-Client-Id`)
- `POST /api/v1/checkout` â€“ Stripe Checkout Session erstellen `{ quantity?: number }` (Header `X-Client-Id`)
- `POST /api/v1/stripe/webhook` â€“ Stripe Fulfillment Webhook (signiert, idempotent)

Hinweise:
- Clients identifizieren sich anonym via `X-Client-Id` (8â€“64 Zeichen, `[A-Za-z0-9_-]`). Keine PII speichern.
- Neue Clients erhalten automatisch 5 Start-Tokens.
- Beim Checkout werden `clientId` und `quantity` in Stripe `metadata` gespeichert.
- Nach erfolgreicher Zahlung erhÃ¶ht der Webhook das Guthaben idempotent.

### Image Generation
- `POST /api/v1/images/generate` â€“ Text-/Image-to-Image via Replicate (minimax/image-01)
- `POST /api/v1/generate` â€“ Preset-basierte Generierung via `bubbles[]` + optional `notes` + optional `image` (Header `X-Client-Id`, Tokenabzug je Bild; bei `framing=collection` Standardmenge 3)
- `GET /api/v1/images/latest-random?limit=20` â€“ Neueste generierte Bilder (vom Image-Server), zufÃ¤llig gemischt
- `GET /api/v1/images/random` â€“ 20 zufÃ¤llige generierte Bilder

### Document Management
- `POST /api/v1/documents/upload` - Dokument hochladen
- `GET /api/v1/documents` - Alle Dokumente auflisten
- `GET /api/v1/documents/stats` - Storage Statistiken
- `GET /api/v1/documents/:id` - Spezifisches Dokument
- `PUT /api/v1/documents/:id` - Dokument aktualisieren
- `DELETE /api/v1/documents/:id` - Dokument lÃ¶schen

### Query & Search
- `POST /api/v1/query` - Frage stellen (mit AI Antwort)
- `POST /api/v1/query/similar` - Ã„hnliche Dokumente finden
- `GET /api/v1/query/explain` - Query-Prozess erklÃ¤ren
- `GET /api/v1/query/stats` - Query Statistiken

### Cannabis Strain Management & Recommendations ğŸŒ¿
- `POST /api/v1/cannabis/strains` - Cannabis Strain hinzufÃ¼gen
- `POST /api/v1/cannabis/recommendations` - Mood-basierte Strain-Empfehlungen
- `POST /api/v1/cannabis/strain-recommendations` - Stimmungsbasierte Strain-Empfehlungen mit KI-Text
- `GET /api/v1/cannabis/strains` - Alle Strains auflisten
- `DELETE /api/v1/cannabis/strains/:id` - Strain lÃ¶schen
- `GET /api/v1/cannabis/health` - Cannabis Service Health Check
- `GET /api/v1/cannabis/stats` - Cannabis Knowledge Base Statistiken

## ğŸ”§ Usage Examples

### Dokument hochladen

```bash
curl -X POST "http://localhost:3000/api/v1/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf" \
  -F "name=My Document" \
  -F "description=Important project document" \
  -F "tags=project,documentation"

# Oder fÃ¼r Bildupload mit OCR
curl -X POST "http://localhost:3000/api/v1/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@screenshot.png" \
  -F "name=Screenshot Analysis" \
  -F "description=OCR text extraction from image" \
  -F "tags=image,ocr,analysis"
```

### Frage stellen

```bash
curl -X POST "http://localhost:3000/api/v1/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Was sind die Hauptthemen in den Dokumenten?",
    "maxResults": 5,
    "minScore": 0.7
  }'
```

### Image Generation

1) Einfacher Prompt (optional mit Referenzbild â€“ wird als `subject_reference` an Replicate Ã¼bergeben)

```bash
http -f POST :3000/api/v1/images/generate \
  prompt=="A close-up portrait of a leopard" \
  n:=2 \
  aspect_ratio=="3:4" \
  image@./example.jpg

# Erwartete Antwort:
# { "images": ["https://hel1.your-objectstorage.com/<bucket>/generated/<file>.jpg", "..."] }
```

2) Preset-driven Endpoint mit bubbles + notes (+ optional amount, Default 1), plus Styling-Optionen (gender, hairstyle, hairColor, framing)

```bash
http -f POST :3000/api/v1/generate \
  bubbles:='["editorial_studio","gym_fit"]' \
  notes=="slight smile, clean minimal background" \
  gender=="male" \
  hairstyleId=="curly_volume" \
  hairColorFrom=="#5A3E36" \
  hairColorTo=="#5A3E36" \
  framing=="face" \
  amount:=1 \
  image@./reference.jpg

# Antwort: { "images": ["https://...", ...], "bubbles": ["editorial_studio", "gym_fit"] }
```

3) Neueste generierte Bilder (random)

```bash
http GET :3000/api/v1/images/latest-random limit==24

# Antwort: { "images": ["https://hel1.your-objectstorage.com/<bucket>/generated/<file>.jpg", ...] }
```

4) 20 zufÃ¤llige generierte Bilder

```bash
http GET :3000/api/v1/images/random

# Antwort: { "images": ["https://hel1.your-objectstorage.com/<bucket>/generated/<file>.jpg", ...] }
```

TypeScript/JS Client-Beispiel

```ts
import type { Bubble } from './types';

export type GenerateRequest = {
  bubbles: string[];
  notes?: string;
  image?: File | Blob;
};

export interface GenerateResponse { images: string[] }
type ApiError = { error?: string };

export async function generateImages(baseUrl: string, request: GenerateRequest): Promise<GenerateResponse> {
  const formData = new FormData();
  if (request.image) {
    formData.append('image', request.image);
  }
  if (request.notes) {
    formData.append('notes', request.notes);
  }
  formData.append('bubbles', JSON.stringify(request.bubbles));

  const response = await fetch(`${baseUrl}/generate`, { method: 'POST', body: formData });
  if (!response.ok) {
    const error: ApiError = await response.json().catch(() => ({} as ApiError));
    throw new Error(error?.error || 'Generation failed');
  }
  return response.json();
}

// Tipp: baseUrl sollte auf die API-PrÃ¤fix-Route zeigen, z. B.:
// const baseUrl = 'http://localhost:3000/api/v1';
```

### Ã„hnliche Dokumente finden

```bash
curl -X POST "http://localhost:3000/api/v1/query/similar" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "machine learning algorithms",
    "limit": 10,
    "minScore": 0.6
  }'
```

### Cannabis Strain hinzufÃ¼gen ğŸŒ¿

```bash
curl -X POST "http://localhost:3000/api/v1/cannabis/strains" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Blue Dream",
    "type": "hybrid",
    "description": "A balanced hybrid strain known for its sweet berry aroma and relaxing effects",
    "thc": 18.5,
    "cbd": 0.1,
    "effects": ["happy", "relaxed", "euphoric", "creative"],
    "flavors": ["berry", "sweet", "vanilla"],
    "medical": ["stress", "depression", "pain"],
    "terpenes": [
      {"name": "Myrcene", "percentage": 0.8},
      {"name": "Limonene", "percentage": 0.6}
    ],
    "genetics": "Blueberry x Haze",
    "breeder": "DJ Short",
    "rating": 4.2
  }'
```

### Mood-basierte Strain-Empfehlungen

```bash
curl -X POST "http://localhost:3000/api/v1/cannabis/recommendations" \
  -H "Content-Type: application/json" \
  -d '{
    "moodDescription": "I feel stressed after a long day at work and want to relax while watching Netflix",
    "timeOfDay": "evening",
    "activityContext": "relaxation",
    "targetSymptoms": ["stress", "anxiety"],
    "stressLevel": 8,
    "energyLevel": 3,
    "maxResults": 5,
    "minScore": 0.7
  }'
```

### Alle Cannabis Strains auflisten

```bash
curl -X GET "http://localhost:3000/api/v1/cannabis/strains" \
  -H "Content-Type: application/json"
```

### Cannabis Service Health Check

```bash
curl -X GET "http://localhost:3000/api/v1/cannabis/health" \
  -H "Content-Type: application/json"
```

### Cannabis Strain lÃ¶schen

```bash
curl -X DELETE "http://localhost:3000/api/v1/cannabis/strains/strain-uuid-123" \
  -H "Content-Type: application/json"
```

### Cannabis Knowledge Base Statistiken

```bash
curl -X GET "http://localhost:3000/api/v1/cannabis/stats" \
  -H "Content-Type: application/json"
```

### Stimmungsbasierte Strain-Empfehlungen mit KI-Text ğŸŒ¿ğŸ¤–

```bash
curl -X POST "http://localhost:3000/api/v1/cannabis/strain-recommendations" \
  -H "Content-Type: application/json" \
  -d '{
    "moodDescription": "Ich fÃ¼hle mich gestresst nach der Arbeit und mÃ¶chte entspannen",
    "maxResults": 5
  }'
```

**Weitere Beispiele fÃ¼r Stimmungsbeschreibungen:**
```bash
# Kreative Session
curl -X POST "http://localhost:3000/api/v1/cannabis/strain-recommendations" \
  -H "Content-Type: application/json" \
  -d '{"moodDescription": "Ich mÃ¶chte kreativ sein und an meinem Kunstprojekt arbeiten"}'

# Entspannung am Abend
curl -X POST "http://localhost:3000/api/v1/cannabis/strain-recommendations" \
  -H "Content-Type: application/json" \
  -d '{"moodDescription": "MÃ¼de vom Tag, brauche Entspannung fÃ¼r Netflix & Chill"}'

# Energie fÃ¼r den Tag
curl -X POST "http://localhost:3000/api/v1/cannabis/strain-recommendations" \
  -H "Content-Type: application/json" \
  -d '{"moodDescription": "Morgens, brauche Energie und Fokus fÃ¼r die Arbeit"}'
```

**Antwort-Beispiel:**
```json
{
  "moodAnalysis": {
    "detectedMood": "gestresst, entspannung suchend",
    "recommendedEffects": ["relaxed", "calm", "stress-relief"],
    "timeContext": "nach der Arbeit",
    "intensity": "medium",
    "strainType": "indica",
    "keywords": ["stress-relief", "relaxation"]
  },
  "strains": [
    {
      "id": "strain-uuid-123",
      "name": "Blue Dream",
      "type": "hybrid",
      "description": "A balanced hybrid strain known for its sweet berry aroma...",
      "thc": 18.5,
      "cbd": 0.2,
      "effects": ["happy", "relaxed", "creative"],
      "flavors": ["berry", "sweet", "vanilla"],
      "medical": ["stress", "depression", "pain"],
      "terpenes": [
        {"name": "Myrcene", "percentage": 0.8},
        {"name": "Limonene", "percentage": 0.6}
      ],
      "genetics": "Blueberry x Haze",
      "rating": 4.2,
      "recommendationText": "FÃ¼r deine stressige Situation nach der Arbeit ist Blue Dream genau das Richtige. Die ausgewogene Hybrid-Genetik hilft dir dabei, vom Arbeitsstress abzuschalten und gleichzeitig entspannt aber nicht mÃ¼de zu werden. Mit seinen entspannenden Effekten ist es perfekt fÃ¼r dein After-Work-Ritual.",
      "similarity": 0.89,
      "matchReason": "Perfekt fÃ¼r gestresst, entspannung suchend - bietet relaxed, calm",
      "createdAt": "2025-09-15T10:00:00.000Z"
    }
  ],
  "totalResults": 5,
  "processingTime": 1250,
  "generatedAt": "2025-09-15T10:30:00.000Z"
}
```

---

## ğŸ§  Cognee Knowledge Graph API

Das Cognee-Modul bietet erweiterte Knowledge-Graph-FunktionalitÃ¤ten zur semantischen Datenverarbeitung und -analyse.

### Textdaten zu Cognee hochladen

```bash
curl -X POST "http://localhost:3000/api/v1/cognee/upload/data" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Cannabis ist eine vielseitige Pflanze mit verschiedenen medizinischen Anwendungen. THC und CBD sind die wichtigsten Cannabinoide.",
    "dataType": "text",
    "title": "Cannabis Grundlagen",
    "processingMode": "full",
    "metadata": {
      "source": "research_document",
      "tags": ["cannabis", "medizin", "forschung"],
      "author": "Dr. Cannabis",
      "createdAt": "2025-09-11"
    },
    "createRelationships": true,
    "extractEntities": true
  }'
```

### Textdatei zu Cognee hochladen

```bash
curl -X POST "http://localhost:3000/api/v1/cognee/upload/file" \
  -F "file=@cannabis_research.txt" \
  -F "author=Prof. Dr. Cannabis Forscher" \
  -F "tags=medizin,cannabis,forschung,wissenschaft"
```

### Cognee Knowledge Graph abfragen

```bash
curl -X POST "http://localhost:3000/api/v1/cognee/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Finde alle Informationen Ã¼ber THC und CBD Wirkmechanismen",
    "limit": 10
  }'
```

### EntitÃ¤ten im Knowledge Graph suchen

```bash
curl -X GET "http://localhost:3000/api/v1/cognee/search?term=cannabis&type=Substance&limit=20" \
  -H "Content-Type: application/json"
```

### Cognee Service Health Check

```bash
curl -X GET "http://localhost:3000/api/v1/cognee/health" \
  -H "Content-Type: application/json"
```

### Cognee Knowledge Graph Statistiken

```bash
curl -X GET "http://localhost:3000/api/v1/cognee/stats" \
  -H "Content-Type: application/json"
```

### UnterstÃ¼tzte Dateiformate fÃ¼r Upload:

- **Text-Dateien:** `.txt`, `.csv`, `.json`, `.md`, `.rtf`
- **Maximale DateigrÃ¶ÃŸe:** 10MB
- **Verarbeitung:** Automatische EntitÃ¤ts-Extraktion und Relationship-Mapping

---

## ğŸŒ¿ğŸ§  Cannabis & Cognee Integration: Wie funktioniert das zusammen?

Diese Anwendung kombiniert **Cannabis-Empfehlungssysteme** mit **Cognee Knowledge Graph-Technologie** fÃ¼r eine umfassende, semantische Datenanalyse. Hier ist eine detaillierte ErklÃ¤rung der Architektur:

### ğŸ”„ **Daten-Pipeline & Workflow:**

#### **1. Cannabis Strain Management (Pinecone + OpenAI)**
```
Cannabis Strain â†’ Text Embedding â†’ Pinecone Vektor DB â†’ AI Mood Analysis â†’ Recommendation
```

**Funktionsweise:**
- Cannabis-Strains werden mit detaillierten Informationen (THC/CBD, Terpene, Effekte) gespeichert
- Jeder Strain wird in einen **hochdimensionalen Vektor** umgewandelt (OpenAI Embeddings)
- Nutzer-Stimmung wird von **GPT-4** analysiert und in semantischen Text konvertiert
- **Vektor-Ã„hnlichkeitssuche** findet passende Strains basierend auf Mood-Profil

#### **2. Cognee Knowledge Graph (Advanced Semantics)**
```
Research Data â†’ Entity Extraction â†’ Relationship Mapping â†’ Knowledge Graph â†’ Query Interface
```

**Funktionsweise:**
- Forschungsdaten und Texte werden hochgeladen
- **EntitÃ¤ts-Extraktion** identifiziert Cannabis-relevante Begriffe (THC, CBD, Terpene, Symptome)
- **Beziehungs-Mapping** erstellt semantische Verbindungen zwischen EntitÃ¤ten
- **NatÃ¼rlichsprachliche Queries** durchsuchen den Knowledge Graph

### ğŸ”— **Synergie-Effekte:**

#### **A) Enhanced Strain Discovery**
```mermaid
graph LR
    A[User Mood] --> B[GPT Analysis]
    B --> C[Vector Search]
    C --> D[Strain Matches]
    D --> E[Cognee Context]
    E --> F[Enhanced Recommendations]
```

- Cannabis-System findet Basis-Matches
- Cognee liefert zusÃ¤tzlichen **wissenschaftlichen Kontext**
- Kombinierte Empfehlungen sind **prÃ¤ziser und informierter**

#### **B) Research-Driven Recommendations**
```mermaid
graph TD
    A[Research Papers] --> B[Cognee Processing]
    B --> C[Entity Graph]
    C --> D[Strain Database]
    D --> E[Evidence-Based Recommendations]
```

- Wissenschaftliche Studien werden in Cognee verarbeitet
- Extrahierte Erkenntnisse **validieren Cannabis-Empfehlungen**
- Nutzer erhalten **evidenz-basierte BegrÃ¼ndungen**

### ğŸ¯ **Praktische AnwendungsfÃ¤lle:**

#### **Szenario 1: Mood-basierte Empfehlung mit Wissenschaftskontext**
```bash
# 1. Strain-Empfehlung basierend auf Stimmung
curl -X POST "/api/v1/cannabis/recommendations" -d '{
  "moodDescription": "Stressed from work, need relaxation",
  "timeOfDay": "evening"
}'

# 2. ZusÃ¤tzlicher Kontext aus Cognee Knowledge Graph
curl -X POST "/api/v1/cognee/query" -d '{
  "query": "Stress-relief mechanisms of recommended terpenes"
}'
```

#### **Szenario 2: Forschung-zu-Praxis Pipeline**
```bash
# 1. Neue Forschung zu Cognee hinzufÃ¼gen
curl -X POST "/api/v1/cognee/upload/file" \
  -F "file=@new_cannabis_study.txt" \
  -F "tags=research,clinical-trial"

# 2. Erkenntnisse in Cannabis-Empfehlungen integrieren
curl -X POST "/api/v1/cannabis/recommendations" -d '{
  "moodDescription": "Looking for evidence-based anxiety relief"
}'
```

### ğŸ—ï¸ **Technische Architektur:**

#### **Datenfluss:**
1. **Input Layer**: User Mood/Research Data
2. **Processing Layer**: GPT Analysis/Cognee Entity Extraction  
3. **Storage Layer**: Pinecone Vectors/Knowledge Graph
4. **Retrieval Layer**: Semantic Search/Graph Queries
5. **Output Layer**: Enhanced Recommendations/Context

#### **Shared Components:**
- **OpenAI GPT-4**: Mood Analysis + Entity Reasoning
- **Vector Embeddings**: Semantic Similarity fÃ¼r beide Module
- **NestJS Framework**: Unified API Architecture
- **TypeScript DTOs**: Type-safe Data Flow

### ğŸ”¬ **Wissenschaftlicher Mehrwert:**

#### **Evidence-Based Cannabis Recommendations:**
```typescript
interface EnhancedRecommendation {
  strain: CannabiStrain;
  moodMatch: number;
  scientificEvidence: {
    studies: ResearchPaper[];
    mechanisms: string[];
    contraindications: string[];
  };
  cogneeInsights: {
    relatedEntities: Entity[];
    supportingRelationships: Relationship[];
  };
}
```

#### **Dynamisches Lernen:**
- Neue Forschung **aktualisiert automatisch** Empfehlungslogik
- Knowledge Graph **erweitert sich kontinuierlich**
- **Feedback-Loops** verbessern Accuracy Ã¼ber Zeit

### ğŸš€ **Erweiterte Use Cases:**

1. **Medical Professional Dashboard**: Ã„rzte kÃ¶nnen evidenz-basierte Cannabis-Verschreibungen machen
2. **Research Platform**: Wissenschaftler kÃ¶nnen neue Erkenntnisse mit existierenden Strain-Daten verknÃ¼pfen  
3. **Patient Journey Tracking**: Langzeit-Outcomes mit wissenschaftlichen Erkenntnissen korrelieren
4. **Regulatory Compliance**: Automatische Generierung von Sicherheits- und Wirksamkeitsberichten

### ğŸ’¡ **Warum diese Kombination?**

**Cannabis allein**: Gute Empfehlungen basierend auf Nutzer-PrÃ¤ferenzen
**Cognee allein**: MÃ¤chtige semantische Datenanalyse ohne DomÃ¤nen-Fokus
**Cannabis + Cognee**: **Wissenschaftlich validierte, personalisierte Cannabis-Medizin**

Diese Integration macht aus subjektiven Empfehlungen **objektive, datengetriebene Entscheidungshilfen** fÃ¼r medizinisches Cannabis.

---

## ğŸ§ª Testing

```bash
# Unit Tests
npm run test

# E2E Tests
npm run test:e2e

# Test Coverage
npm run test:cov
```

## ğŸ“ Development

### Code Formatting & Linting

```bash
# Format code
npm run format

# Lint code
npm run lint

# Fix linting issues
npm run lint:fix
```

### Debugging

```bash
# Start in debug mode
npm run start:debug
```

Debug-Port: `9229`

## ğŸ—ï¸ Architecture

### Text Processing Pipeline

1. **File Upload**: PDF/TXT/DOCX/Bilddateien werden akzeptiert
2. **Text Extraction**: 
   - PDF: Direkter Text-Export mit pdf-parse
   - DOCX: Text-Extraktion mit mammoth
   - Bilder: OCR mit Tesseract.js (Deutsch + Englisch)
   - Bildvorverarbeitung mit Sharp fÃ¼r bessere OCR-Ergebnisse
3. **Text Chunking**: Dokumente werden in Ã¼berlappende Chunks aufgeteilt (700 Zeichen, 100 Zeichen Ãœberlappung)
4. **Embeddings Generation**: Jeder Chunk wird mit OpenAI's text-embedding-ada-002 in Vektoren umgewandelt
5. **Vector Storage**: Vektoren werden in Pinecone gespeichert mit Metadaten

### Query Processing Pipeline

1. **Question Embedding**: Frage wird in Vektor umgewandelt
2. **Similarity Search**: Pinecone findet Ã¤hnlichste Dokument-Chunks
3. **Context Filtering**: Chunks werden nach Relevanz-Score gefiltert
4. **Response Generation**: OpenAI GPT generiert Antwort basierend auf Kontext
5. **Response Enrichment**: Antwort wird mit Quellen und Confidence-Score angereichert

## ğŸ”’ Security Considerations

- **Rate Limiting**: Automatischer Schutz vor API-Missbrauch
- **Input Validation**: Alle Eingaben werden validiert
- **File Upload Restrictions**: Nur erlaubte Dateitypen und GrÃ¶ÃŸenbeschrÃ¤nkungen
- **Environment Variables**: Sensible Daten in .env Dateien
- **CORS**: Konfigurierbare Cross-Origin Requests

## ğŸ“Š Monitoring & Logging

Das System bietet umfassendes Logging fÃ¼r:
- API Request/Response Zeiten
- Token Usage Tracking
- Embedding Generation Performance
- Pinecone Query Performance
- Error Rates und Types

## ğŸš€ Production Deployment

### Docker Deployment

```bash
# Build Docker Image
docker build -t rag-backend .

# Run Container
docker run -p 3000:3000 --env-file .env rag-backend
```

### Environment Variables fÃ¼r Production

```env
NODE_ENV=production
PORT=3000

# Security
THROTTLE_SHORT_LIMIT=5
THROTTLE_MEDIUM_LIMIT=50
THROTTLE_LONG_LIMIT=500

# Logging
LOG_LEVEL=info
```

## ğŸ“ˆ Performance Optimization

- **Batch Processing**: Embeddings werden in Batches verarbeitet
- **Connection Pooling**: Effiziente Pinecone Verbindungen
- **Caching**: Response Caching fÃ¼r hÃ¤ufige Queries
- **Chunking Strategy**: Optimierte Chunk-GrÃ¶ÃŸen fÃ¼r bessere Retrieval

## ğŸ” Troubleshooting

### HÃ¤ufige Probleme

1. **Pinecone Connection Failed**
   - ÃœberprÃ¼fe API Key und Index Name
   - Stelle sicher, dass Index mit 1536 Dimensionen erstellt wurde

2. **OpenAI API Errors**
   - ÃœberprÃ¼fe API Key GÃ¼ltigkeit
   - ÃœberprÃ¼fe Account Credits/Limits

3. **File Upload Issues**
   - ÃœberprÃ¼fe DateigrÃ¶ÃŸe (max 10MB)
   - UnterstÃ¼tzte Formate: PDF, TXT, DOCX, JPG, PNG, GIF, BMP, TIFF, WEBP

4. **OCR Processing Issues**
   - OCR kann bei schlechter BildqualitÃ¤t fehlschlagen
   - FÃ¼r bessere Ergebnisse: Hoher Kontrast, klare Schrift verwenden
   - Tesseract unterstÃ¼tzt Deutsch und Englisch

5. **Memory Issues**
   - Reduziere Batch-GrÃ¶ÃŸen fÃ¼r Embeddings
   - Ãœberwache Heap Usage bei groÃŸen Dokumenten

## ğŸ¤ Contributing

1. Fork das Repository
2. Erstelle einen Feature Branch (`git checkout -b feature/amazing-feature`)
3. Commite deine Changes (`git commit -m 'Add amazing feature'`)
4. Push zum Branch (`git push origin feature/amazing-feature`)
5. Ã–ffne eine Pull Request

## ğŸ“„ License

Dieses Projekt steht unter der [MIT License](LICENSE).

## ğŸ™‹â€â™‚ï¸ Support

Bei Fragen oder Problemen:
- Erstelle ein [Issue](https://github.com/your-repo/issues)
- Kontaktiere: your-email@example.com

---

**Erstellt mit â¤ï¸ using NestJS, Pinecone, and OpenAI**
